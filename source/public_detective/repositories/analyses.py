"""This module defines the repository for handling database operations related to procurement analysis results."""

import json
from typing import Any, cast
from uuid import UUID

from public_detective.models.analyses import Analysis, AnalysisResult
from public_detective.models.procurement_analysis_status import ProcurementAnalysisStatus
from public_detective.providers.logging import Logger, LoggingProvider
from pydantic import ValidationError
from sqlalchemy import Engine, text


class AnalysisRepository:
    """Handles all database operations related to procurement analyses.

    This repository provides a suite of methods for creating, retrieving,
    and updating records in the `procurement_analyses` table. It abstracts
    the underlying SQL queries and provides a clean, model-centric interface
    to the rest of the application.

    Args:
        engine: An SQLAlchemy Engine instance used to connect to the database.
    """

    logger: Logger
    engine: Engine

    def __init__(self, engine: Engine) -> None:
        """Initializes the repository with a database engine.

        Args:
            engine: The SQLAlchemy Engine to be used for all database
                communications.
        """
        self.logger = LoggingProvider().get_logger()
        self.engine = engine

    def _parse_row_to_model(self, row: tuple, columns: list[str]) -> AnalysisResult | None:
        """Parses a database row into an `AnalysisResult` Pydantic model.

        This private helper method takes a raw tuple-based row from an
        SQLAlchemy result and a list of column names, and attempts to
        construct a validated `AnalysisResult` model from it. It handles
        the parsing of nested JSON data for fields like `red_flags`.

        Args:
            row: A tuple representing a single row from the database.
            columns: A list of column names corresponding to the items in the
                row tuple.

        Returns:
            An instance of `AnalysisResult` if parsing is successful,
            otherwise `None`.
        """
        if not row:
            return None

        row_dict = dict(zip(columns, row))
        red_flags_data = row_dict.get("red_flags")
        if red_flags_data is None:
            red_flags = []
        elif isinstance(red_flags_data, str):
            red_flags = json.loads(red_flags_data)
        else:
            red_flags = red_flags_data

        warnings_data = row_dict.get("warnings")
        if warnings_data is None:
            warnings = []
        else:
            warnings = warnings_data

        try:
            ai_analysis_data = {
                "risk_score": row_dict.get("risk_score") or 0,
                "risk_score_rationale": row_dict.get("risk_score_rationale") or "",
                "procurement_summary": row_dict.get("procurement_summary") or "",
                "analysis_summary": row_dict.get("analysis_summary") or "",
                "red_flags": red_flags,
                "seo_keywords": row_dict.get("seo_keywords") or [],
            }
            row_dict["ai_analysis"] = Analysis.model_validate(ai_analysis_data)
            row_dict["warnings"] = warnings
            row_dict["retry_count"] = row_dict.get("retry_count", 0)
            row_dict["updated_at"] = row_dict.get("updated_at")
            row_dict["votes_count"] = row_dict.get("votes_count", 0)

            return AnalysisResult.model_validate(row_dict)
        except ValidationError as e:
            self.logger.error(f"Failed to parse analysis result from DB due to validation error: {e}")
            return None

    def save_analysis(
        self, analysis_id: UUID, result: AnalysisResult, input_tokens_used: int, output_tokens_used: int
    ) -> None:
        """Updates an existing analysis record with the full analysis results.

        This method populates a `procurement_analyses` record (which was
        previously created by `save_pre_analysis`) with all the detailed
        findings from the AI, along with metadata like GCS paths and token
        counts. It also sets the status to 'ANALYSIS_SUCCESSFUL'.

        Args:
            analysis_id: The ID of the analysis record to update.
            result: The `AnalysisResult` object containing all the data to
                be saved.
            input_tokens_used: The number of input tokens consumed by the AI.
            output_tokens_used: The number of output tokens generated by the AI.
        """
        self.logger.info(f"Updating analysis for analysis_id {analysis_id}.")

        sql = text(
            """
            UPDATE procurement_analyses
            SET
                document_hash = :document_hash,
                risk_score = :risk_score,
                risk_score_rationale = :risk_score_rationale,
                procurement_summary = :procurement_summary,
                analysis_summary = :analysis_summary,
                red_flags = :red_flags,
                seo_keywords = :seo_keywords,
                warnings = :warnings,
                original_documents_gcs_path = :original_documents_gcs_path,
                processed_documents_gcs_path = :processed_documents_gcs_path,
                status = :status,
                input_tokens_used = :input_tokens_used,
                output_tokens_used = :output_tokens_used,
                updated_at = now()
            WHERE analysis_id = :analysis_id;
        """
        )

        red_flags_json = json.dumps([rf.model_dump() for rf in result.ai_analysis.red_flags])

        params = {
            "analysis_id": analysis_id,
            "document_hash": result.document_hash,
            "risk_score": result.ai_analysis.risk_score,
            "risk_score_rationale": result.ai_analysis.risk_score_rationale,
            "procurement_summary": result.ai_analysis.procurement_summary,
            "analysis_summary": result.ai_analysis.analysis_summary,
            "red_flags": red_flags_json,
            "seo_keywords": result.ai_analysis.seo_keywords,
            "warnings": result.warnings,
            "original_documents_gcs_path": result.original_documents_gcs_path,
            "processed_documents_gcs_path": result.processed_documents_gcs_path,
            "status": ProcurementAnalysisStatus.ANALYSIS_SUCCESSFUL.value,
            "input_tokens_used": input_tokens_used,
            "output_tokens_used": output_tokens_used,
        }

        with self.engine.connect() as conn:
            conn.execute(sql, params)
            conn.commit()

        self.logger.info(f"Analysis updated successfully for ID: {analysis_id}.")

    def get_analysis_by_hash(self, document_hash: str) -> AnalysisResult | None:
        """Retrieves a successful analysis by the hash of its document content.

        This method is used for idempotency checks. It searches for a
        previously completed analysis (`ANALYSIS_SUCCESSFUL`) that has the
        exact same document hash, allowing the system to reuse results
        instead of re-processing identical documents.

        Args:
            document_hash: The SHA-256 hash of the document content.

        Returns:
            An `AnalysisResult` object if a matching, successful analysis
            is found, otherwise `None`.
        """
        sql = text(
            """
            SELECT
                analysis_id,
                procurement_control_number,
                version_number,
                status,
                risk_score,
                risk_score_rationale,
                procurement_summary,
                analysis_summary,
                red_flags,
                seo_keywords,
                warnings,
                document_hash,
                original_documents_gcs_path,
                processed_documents_gcs_path,
                input_tokens_used,
                output_tokens_used,
                created_at,
                updated_at,
                retry_count,
                votes_count
            FROM procurement_analyses
            WHERE document_hash = :document_hash AND status = :status
            LIMIT 1;
            """
        )

        with self.engine.connect() as conn:
            result = conn.execute(
                sql,
                {
                    "document_hash": document_hash,
                    "status": ProcurementAnalysisStatus.ANALYSIS_SUCCESSFUL.value,
                },
            ).fetchone()
            if not result:
                return None
            columns = list(result._fields)
            row = tuple(result)

        return self._parse_row_to_model(row, columns)

    def save_pre_analysis(
        self,
        procurement_control_number: str,
        version_number: int,
        document_hash: str,
        input_tokens_used: int,
        output_tokens_used: int,
    ) -> UUID:
        """Saves a new, pending analysis record to the database.

        This method creates the initial record in the `procurement_analyses`
        table. This record acts as a placeholder, indicating that a
        procurement has been identified for analysis but has not yet been
        fully processed. It sets the initial status to 'PENDING_ANALYSIS'.

        Args:
            procurement_control_number: The control number of the associated
                procurement.
            version_number: The version of the procurement being analyzed.
            document_hash: The hash of the documents selected for analysis.
            input_tokens_used: The estimated number of input tokens.
            output_tokens_used: The estimated number of output tokens.

        Returns:
            The newly created `analysis_id` for the record.
        """
        self.logger.info(f"Saving pre-analysis for {procurement_control_number} version {version_number}.")
        sql = text(
            """
            INSERT INTO procurement_analyses (
                procurement_control_number, version_number, status, document_hash,
                input_tokens_used, output_tokens_used
            ) VALUES (
                :procurement_control_number, :version_number, :status, :document_hash,
                :input_tokens_used, :output_tokens_used
            )
            RETURNING analysis_id;
            """
        )
        params = {
            "procurement_control_number": procurement_control_number,
            "version_number": version_number,
            "document_hash": document_hash,
            "status": ProcurementAnalysisStatus.PENDING_ANALYSIS.value,
            "input_tokens_used": input_tokens_used,
            "output_tokens_used": output_tokens_used,
        }
        with self.engine.connect() as conn:
            result_proxy = conn.execute(sql, params)
            analysis_id = cast(UUID, result_proxy.scalar_one())
            conn.commit()
        self.logger.info(f"Pre-analysis saved successfully with ID: {analysis_id}.")
        return analysis_id

    def get_analysis_by_id(self, analysis_id: UUID) -> AnalysisResult | None:
        """Retrieves a single analysis record by its primary key.

        Args:
            analysis_id: The unique ID of the analysis to retrieve.

        Returns:
            An `AnalysisResult` object if found, otherwise `None`.
        """
        sql = text(
            """
            SELECT
                analysis_id,
                procurement_control_number,
                version_number,
                status,
                risk_score,
                risk_score_rationale,
                procurement_summary,
                analysis_summary,
                red_flags,
                seo_keywords,
                warnings,
                document_hash,
                original_documents_gcs_path,
                processed_documents_gcs_path,
                input_tokens_used,
                output_tokens_used,
                created_at,
                updated_at,
                retry_count,
                votes_count
            FROM procurement_analyses
            WHERE analysis_id = :analysis_id
            LIMIT 1;
            """
        )

        with self.engine.connect() as conn:
            result = conn.execute(sql, {"analysis_id": analysis_id}).fetchone()
            if not result:
                return None
            columns = list(result._fields)
            row = tuple(result)

        return self._parse_row_to_model(row, columns)

    def update_analysis_status(self, analysis_id: UUID, status: ProcurementAnalysisStatus) -> None:
        """Updates the status of a specific analysis record.

        This is used to track the lifecycle of an analysis, for example,
        moving it from 'PENDING_ANALYSIS' to 'ANALYSIS_IN_PROGRESS' or
        from 'ANALYSIS_IN_PROGRESS' to 'ANALYSIS_SUCCESSFUL'.

        Args:
            analysis_id: The ID of the analysis to update.
            status: The new status to set for the analysis.
        """
        self.logger.info(f"Updating status for analysis {analysis_id} to {status}.")
        sql = text(
            """
            UPDATE procurement_analyses
            SET status = :status, updated_at = now()
            WHERE analysis_id = :analysis_id;
            """
        )
        with self.engine.connect() as conn:
            conn.execute(sql, {"analysis_id": analysis_id, "status": status.value})
            conn.commit()
        self.logger.info("Analysis status updated successfully.")

    def get_analyses_to_retry(self, max_retries: int, timeout_hours: int) -> list[AnalysisResult]:
        """Retrieves a list of analyses that are eligible for a retry attempt.

        This method identifies analyses that have failed or have been stuck in
        progress for too long, and have not yet exceeded the maximum number of
        retry attempts.

        Args:
            max_retries: The maximum number of retries allowed for an analysis.
            timeout_hours: The number of hours after which an 'IN_PROGRESS' task
                is considered stale.

        Returns:
            A list of `AnalysisResult` objects that are eligible for retry.
        """
        self.logger.info("Fetching analyses to retry...")
        sql = text(
            """
            SELECT
                analysis_id,
                procurement_control_number,
                version_number,
                status,
                retry_count,
                risk_score,
                risk_score_rationale,
                procurement_summary,
                analysis_summary,
                red_flags,
                seo_keywords,
                warnings,
                document_hash,
                original_documents_gcs_path,
                processed_documents_gcs_path,
                input_tokens_used,
                output_tokens_used,
                created_at,
                updated_at,
                votes_count
            FROM procurement_analyses
            WHERE
                (
                    status = :failed_status
                    OR (
                        status = :in_progress_status
                        AND updated_at < NOW() - (INTERVAL '1 hour' * :timeout_hours)
                    )
                )
                AND retry_count < :max_retries;
            """
        )
        params = {
            "failed_status": ProcurementAnalysisStatus.ANALYSIS_FAILED.value,
            "in_progress_status": ProcurementAnalysisStatus.ANALYSIS_IN_PROGRESS.value,
            "timeout_hours": timeout_hours,
            "max_retries": max_retries,
        }
        with self.engine.connect() as conn:
            result = conn.execute(sql, params).fetchall()

        if not result:
            return []

        columns = list(result[0]._fields)
        analyses = [self._parse_row_to_model(tuple(row), columns) for row in result]
        return [analysis for analysis in analyses if analysis]

    def save_retry_analysis(
        self,
        procurement_control_number: str,
        version_number: int,
        document_hash: str,
        input_tokens_used: int,
        output_tokens_used: int,
        retry_count: int,
    ) -> UUID:
        """Saves a new, pending analysis record for a retry attempt.

        This method creates a new analysis record with an incremented retry
        count, setting its status to 'PENDING_ANALYSIS'.

        Args:
            procurement_control_number: The control number of the associated
                procurement.
            version_number: The version of the procurement being analyzed.
            document_hash: The hash of the documents selected for analysis.
            input_tokens_used: The estimated number of input tokens.
            output_tokens_used: The estimated number of output tokens.
            retry_count: The new retry count for this analysis attempt.

        Returns:
            The newly created `analysis_id` for the record.
        """
        self.logger.info(
            f"Saving new retry analysis for {procurement_control_number} "
            f"version {version_number} (attempt {retry_count})."
        )
        sql = text(
            """
            INSERT INTO procurement_analyses (
                procurement_control_number, version_number, status, document_hash,
                input_tokens_used, output_tokens_used, retry_count
            ) VALUES (
                :procurement_control_number, :version_number, :status, :document_hash,
                :input_tokens_used, :output_tokens_used, :retry_count
            )
            RETURNING analysis_id;
            """
        )
        params = {
            "procurement_control_number": procurement_control_number,
            "version_number": version_number,
            "document_hash": document_hash,
            "status": ProcurementAnalysisStatus.PENDING_ANALYSIS.value,
            "input_tokens_used": input_tokens_used,
            "output_tokens_used": output_tokens_used,
            "retry_count": retry_count,
        }
        with self.engine.connect() as conn:
            result_proxy = conn.execute(sql, params)
            analysis_id = cast(UUID, result_proxy.scalar_one())
            conn.commit()
        self.logger.info(f"Retry analysis saved successfully with ID: {analysis_id}.")
        return analysis_id

    def get_pending_analyses_ranked(self) -> list[AnalysisResult]:
        """Retrieves all pending analyses, ranked by votes and cost.

        This method fetches all analyses with the 'PENDING_ANALYSIS' status,
        ordering them first by the number of votes in descending order, and
        then by the estimated input tokens (as a proxy for cost) in
        ascending order.

        Returns:
            A list of `AnalysisResult` objects for the pending analyses.
        """
        self.logger.info("Fetching pending analyses, ranked by votes and cost...")
        sql = text(
            """
            SELECT
                analysis_id,
                procurement_control_number,
                version_number,
                status,
                retry_count,
                risk_score,
                risk_score_rationale,
                procurement_summary,
                analysis_summary,
                red_flags,
                seo_keywords,
                warnings,
                document_hash,
                original_documents_gcs_path,
                processed_documents_gcs_path,
                input_tokens_used,
                output_tokens_used,
                created_at,
                updated_at,
                votes_count
            FROM procurement_analyses
            WHERE status = :pending_status
            ORDER BY votes_count DESC, input_tokens_used ASC;
            """
        )
        params = {"pending_status": ProcurementAnalysisStatus.PENDING_ANALYSIS.value}
        with self.engine.connect() as conn:
            result = conn.execute(sql, params).fetchall()

        if not result:
            return []

        columns = list(result[0]._fields)
        analyses = [self._parse_row_to_model(tuple(row), columns) for row in result]
        return [analysis for analysis in analyses if analysis]

    def get_procurement_overall_status(self, procurement_control_number: str) -> dict[str, Any] | None:
        """Retrieves the overall status of a procurement based on its analysis history.

        This method executes a complex query that determines the single, most relevant
        status for a procurement, considering all its versions and analysis states.

        Args:
            procurement_control_number: The unique control number of the procurement.

        Returns:
            A dictionary containing the 'procurement_id', 'latest_version', and
            'overall_status', or None if the procurement is not found.
        """
        sql = text(  # pragma: no cover
            """
            WITH latest_procurement AS (
              SELECT
                pncp_control_number,
                MAX(version_number) AS latest_version
              FROM procurements
              WHERE pncp_control_number = :pncp_control_number
              GROUP BY pncp_control_number
            ),
            analysis_status_per_version AS (
              SELECT
                procurement_analyses.procurement_control_number,
                procurement_analyses.version_number,
                BOOL_OR(procurement_analyses.status::text = 'ANALYSIS_SUCCESSFUL') AS version_has_success,
                BOOL_OR(procurement_analyses.status::text = 'ANALYSIS_IN_PROGRESS') AS version_has_in_progress,
                BOOL_OR(procurement_analyses.status::text = 'ANALYSIS_FAILED')     AS version_has_failed,
                BOOL_OR(procurement_analyses.status::text = 'PENDING_ANALYSIS')    AS version_has_pending
              FROM procurement_analyses
              WHERE procurement_analyses.procurement_control_number = :pncp_control_number
              GROUP BY
                procurement_analyses.procurement_control_number,
                procurement_analyses.version_number
            ),
            any_previous_version_analyzed AS (
              SELECT
                latest_procurement.pncp_control_number,
                BOOL_OR(analysis_status_per_version.version_has_success) AS has_success_in_previous_versions
              FROM latest_procurement
              JOIN analysis_status_per_version
                ON analysis_status_per_version.procurement_control_number = latest_procurement.pncp_control_number
               AND analysis_status_per_version.version_number < latest_procurement.latest_version
              GROUP BY latest_procurement.pncp_control_number
            ),
            latest_version_status_rollup AS (
              SELECT
                latest_procurement.pncp_control_number,
                latest_procurement.latest_version,
                COALESCE(analysis_status_per_version.version_has_success, false)     AS latest_version_has_success,
                COALESCE(analysis_status_per_version.version_has_in_progress, false) AS latest_version_has_in_progress,
                COALESCE(analysis_status_per_version.version_has_failed, false)      AS latest_version_has_failed,
                COALESCE(analysis_status_per_version.version_has_pending, false)     AS latest_version_has_pending
              FROM latest_procurement
              LEFT JOIN analysis_status_per_version
                ON analysis_status_per_version.procurement_control_number = latest_procurement.pncp_control_number
               AND analysis_status_per_version.version_number = latest_procurement.latest_version
            )
            SELECT
              latest_version_status_rollup.pncp_control_number AS procurement_id,
              latest_version_status_rollup.latest_version,
              CASE
                WHEN latest_version_status_rollup.latest_version_has_in_progress THEN 'ANALYSIS_IN_PROGRESS'
                WHEN latest_version_status_rollup.latest_version_has_success     THEN 'ANALYZED_CURRENT'
                WHEN latest_version_status_rollup.latest_version_has_failed      THEN 'FAILED_CURRENT'
                WHEN
                    any_previous_version_analyzed.has_success_in_previous_versions IS TRUE
                THEN 'ANALYZED_OUTDATED'
                WHEN
                    latest_version_status_rollup.latest_version_has_pending OR
                    latest_version_status_rollup.latest_version IS NOT NULL
                THEN 'PENDING'
                ELSE 'NOT_ANALYZED'
              END AS overall_status
            FROM latest_version_status_rollup
            LEFT JOIN any_previous_version_analyzed
              ON any_previous_version_analyzed.pncp_control_number = latest_version_status_rollup.pncp_control_number;
            """
        )

        with self.engine.connect() as conn:
            result = conn.execute(sql, {"pncp_control_number": procurement_control_number}).fetchone()

        if not result:
            return None

        return dict(result._mapping)
