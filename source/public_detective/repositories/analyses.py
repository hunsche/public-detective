"""This module defines the repository for handling database operations related to procurement analysis results."""

import json
from decimal import Decimal
from typing import Any, cast
from uuid import UUID

from public_detective.models.analyses import Analysis, AnalysisResult
from public_detective.models.procurement_analysis_status import ProcurementAnalysisStatus
from public_detective.providers.logging import Logger, LoggingProvider
from pydantic import ValidationError
from sqlalchemy import Engine, text


class AnalysisRepository:
    """Handles all database operations related to procurement analyses.

    This repository provides a suite of methods for creating, retrieving,
    and updating records in the `procurement_analyses` table. It abstracts
    the underlying SQL queries and provides a clean, model-centric interface
    to the rest of the application.

    Args:
        engine: An SQLAlchemy Engine instance used to connect to the database.
    """

    logger: Logger
    engine: Engine

    def __init__(self, engine: Engine) -> None:
        """Initializes the repository with a database engine.

        Args:
            engine: The SQLAlchemy Engine to be used for all database
                communications.
        """
        self.logger = LoggingProvider().get_logger()
        self.engine = engine

    def _parse_row_to_model(self, row: tuple, columns: list[str]) -> AnalysisResult | None:
        """Parses a database row into an `AnalysisResult` Pydantic model.

        This private helper method takes a raw tuple-based row from an
        SQLAlchemy result and a list of column names, and attempts to
        construct a validated `AnalysisResult` model from it. It handles
        the parsing of nested JSON data for fields like `red_flags`.

        Args:
            row: A tuple representing a single row from the database.
            columns: A list of column names corresponding to the items in the
                row tuple.

        Returns:
            An instance of `AnalysisResult` if parsing is successful,
            otherwise `None`.
        """
        if not row:
            return None

        row_dict = dict(zip(columns, row))
        red_flags_data = row_dict.get("red_flags")
        if red_flags_data is None:
            red_flags = []
        elif isinstance(red_flags_data, str):
            red_flags = json.loads(red_flags_data)
        else:
            red_flags = red_flags_data

        try:
            ai_analysis_data = {
                "risk_score": row_dict.get("risk_score") or 0,
                "risk_score_rationale": row_dict.get("risk_score_rationale") or "",
                "procurement_summary": row_dict.get("procurement_summary") or "",
                "analysis_summary": row_dict.get("analysis_summary") or "",
                "red_flags": red_flags,
                "seo_keywords": row_dict.get("seo_keywords") or [],
            }
            row_dict["ai_analysis"] = Analysis.model_validate(ai_analysis_data)
            row_dict["retry_count"] = row_dict.get("retry_count", 0)
            row_dict["created_at"] = row_dict.get("created_at")
            row_dict["updated_at"] = row_dict.get("updated_at")
            row_dict["votes_count"] = row_dict.get("votes_count", 0)
            row_dict["grounding_metadata"] = row_dict.get("grounding_metadata")
            row_dict["cost_search_queries"] = row_dict.get("cost_search_queries")
            row_dict["search_queries_used"] = row_dict.get("search_queries_used") or 0
            row_dict["thoughts"] = row_dict.get("thoughts")

            return AnalysisResult.model_validate(row_dict)
        except ValidationError as e:
            self.logger.error(f"Failed to parse analysis result from DB due to validation error: {e}")
            return None

    def save_analysis(
        self,
        analysis_id: UUID,
        result: AnalysisResult,
        input_tokens: int,
        output_tokens: int,
        thinking_tokens: int,
        input_cost: Decimal,
        output_cost: Decimal,
        thinking_cost: Decimal,
        search_cost: Decimal,
        total_cost: Decimal,
        search_queries_used: int = 0,
        analysis_prompt: str = "",
    ) -> None:
        """Updates an existing analysis record with the full analysis results.

        This method populates a `procurement_analyses` record (which was
        previously created by `save_pre_analysis`) with all the detailed
        findings from the AI, along with metadata like GCS paths and token
        counts. It also sets the status to 'ANALYSIS_SUCCESSFUL'.

        Args:
            analysis_id: The ID of the analysis record to update.
            result: The `AnalysisResult` object containing all the data to
                be saved.
            input_tokens: The number of input tokens consumed by the AI.
            output_tokens: The number of output tokens generated by the AI.
            thinking_tokens: The number of thinking tokens used.
            input_cost: The calculated cost of the input tokens.
            output_cost: The calculated cost of the output tokens.
            thinking_cost: The calculated cost of the thinking tokens.
            thinking_cost: The calculated cost of the thinking tokens.
            search_cost: The calculated cost of the search queries.
            total_cost: The total calculated cost of the analysis.
            search_queries_used: The number of search queries performed.
            analysis_prompt: The prompt used for the analysis.
        """
        self.logger.info(f"Updating analysis for analysis_id {analysis_id}.")

        sql = text(
            """
            UPDATE procurement_analyses
            SET
                document_hash = :document_hash,
                risk_score = :risk_score,
                risk_score_rationale = :risk_score_rationale,
                procurement_summary = :procurement_summary,
                analysis_summary = :analysis_summary,
                red_flags = :red_flags,
                seo_keywords = :seo_keywords,
                original_documents_gcs_path = :original_documents_gcs_path,
                processed_documents_gcs_path = :processed_documents_gcs_path,
                status = :status,
                input_tokens_used = :input_tokens_used,
                output_tokens_used = :output_tokens_used,
                thinking_tokens_used = :thinking_tokens_used,
                cost_input_tokens = :cost_input_tokens,
                cost_output_tokens = :cost_output_tokens,
                cost_thinking_tokens = :cost_thinking_tokens,
                cost_search_queries = :cost_search_queries,
                search_queries_used = :search_queries_used,
                total_cost = :total_cost,
                analysis_prompt = :analysis_prompt,
                grounding_metadata = :grounding_metadata,
                thoughts = :thoughts
        WHERE analysis_id = :analysis_id;
        """
        )
        params = {
            "analysis_id": analysis_id,
            "risk_score": result.ai_analysis.risk_score if result.ai_analysis else 0,
            "risk_score_rationale": result.ai_analysis.risk_score_rationale if result.ai_analysis else "",
            "procurement_summary": result.ai_analysis.procurement_summary if result.ai_analysis else "",
            "analysis_summary": result.ai_analysis.analysis_summary if result.ai_analysis else "",
            "red_flags": (
                json.dumps([flag.model_dump(by_alias=True) for flag in result.ai_analysis.red_flags], default=str)
                if result.ai_analysis
                else "[]"
            ),
            "seo_keywords": result.ai_analysis.seo_keywords if result.ai_analysis else [],
            "document_hash": result.document_hash,
            "original_documents_gcs_path": result.original_documents_gcs_path,
            "processed_documents_gcs_path": result.processed_documents_gcs_path,
            "status": ProcurementAnalysisStatus.ANALYSIS_SUCCESSFUL.value,
            "analysis_prompt": result.analysis_prompt,
            "input_tokens_used": input_tokens,
            "output_tokens_used": output_tokens,
            "thinking_tokens_used": thinking_tokens,
            "cost_input_tokens": input_cost,
            "cost_output_tokens": output_cost,
            "cost_thinking_tokens": thinking_cost,
            "cost_search_queries": search_cost,
            "search_queries_used": search_queries_used,
            "total_cost": total_cost,
            "grounding_metadata": (result.grounding_metadata.model_dump_json() if result.grounding_metadata else None),
            "thoughts": result.thoughts,
        }

        with self.engine.connect() as conn:
            conn.execute(sql, params)
            conn.commit()

        self.logger.info(f"Analysis updated successfully for ID: {analysis_id}.")

    def get_analysis_by_hash(self, document_hash: str) -> AnalysisResult | None:
        """Retrieves a successful analysis by the hash of its document content.

        This method is used for idempotency checks. It searches for a
        previously completed analysis (`ANALYSIS_SUCCESSFUL`) that has the
        exact same document hash, allowing the system to reuse results
        instead of re-processing identical documents.

        Args:
            document_hash: The SHA-256 hash of the document content.

        Returns:
            An `AnalysisResult` object if a matching, successful analysis
            is found, otherwise `None`.
        """
        sql = text(
            """
            SELECT
                analysis_id,
                procurement_control_number,
                version_number,
                status,
                risk_score,
                risk_score_rationale,
                procurement_summary,
                analysis_summary,
                red_flags,
                seo_keywords,
                document_hash,
                original_documents_gcs_path,
                processed_documents_gcs_path,
                input_tokens_used,
                output_tokens_used,
                thinking_tokens_used,
                created_at,
                updated_at,
                retry_count,
                votes_count,
                cost_input_tokens,
                cost_output_tokens,
                cost_thinking_tokens,
                cost_search_queries,
                search_queries_used,
                total_cost,
                analysis_prompt,
                thoughts
            FROM procurement_analyses
            WHERE document_hash = :document_hash AND status = :status
            LIMIT 1;
            """
        )

        with self.engine.connect() as conn:
            result = conn.execute(
                sql,
                {
                    "document_hash": document_hash,
                    "status": ProcurementAnalysisStatus.ANALYSIS_SUCCESSFUL.value,
                },
            ).fetchone()
            if not result:
                return None
            columns = list(result._fields)
            row = tuple(result)

        return self._parse_row_to_model(row, columns)

    def get_latest_analysis_with_files(self, procurement_control_number: str, version_number: int) -> UUID | None:
        """Retrieves the ID of the latest analysis for a procurement version that has associated files.

        Args:
            procurement_control_number: The PNCP control number.
            version_number: The specific version number to match.

        Returns:
            The UUID of the analysis with files, or None if not found.
        """
        sql = text(
            """
            SELECT pa.analysis_id
            FROM procurement_analyses pa
            JOIN procurement_source_documents psd ON pa.analysis_id = psd.analysis_id
            JOIN file_records fr ON psd.id = fr.source_document_id
            WHERE pa.procurement_control_number = :control_number
              AND pa.version_number = :version_number
            ORDER BY pa.created_at DESC
            LIMIT 1;
            """
        )
        with self.engine.connect() as conn:
            result = conn.execute(
                sql, {"control_number": procurement_control_number, "version_number": version_number}
            ).scalar_one_or_none()
        return cast(UUID | None, result)

    def create_pre_analysis_record(
        self,
        procurement_control_number: str,
        version_number: int,
        document_hash: str,
        retry_count: int = 0,
    ) -> UUID:
        """Saves a new, pending analysis record to the database.

        This method creates the initial record in the `procurement_analyses`
        table. This record acts as a placeholder, indicating that a
        procurement has been identified for analysis but has not yet been
        fully processed. It sets the initial status to 'PENDING_TOKEN_CALCULATION'.

        Args:
            procurement_control_number: The control number of the associated
                procurement.
            version_number: The version of the procurement being analyzed.
            document_hash: The hash of the documents selected for analysis.
            retry_count: The number of times this analysis has been retried.

        Returns:
            The newly created `analysis_id` for the record.
        """
        self.logger.info(f"Saving pre-analysis for {procurement_control_number} version {version_number}.")
        sql = text(
            """
            INSERT INTO procurement_analyses (
                procurement_control_number, version_number, status, document_hash, retry_count
            ) VALUES (
                :procurement_control_number, :version_number, :status, :document_hash, :retry_count
            )
            RETURNING analysis_id;
            """
        )
        params = {
            "procurement_control_number": procurement_control_number,
            "version_number": version_number,
            "document_hash": document_hash,
            "status": ProcurementAnalysisStatus.PENDING_TOKEN_CALCULATION.value,
            "retry_count": retry_count,
        }
        with self.engine.connect() as conn:
            result_proxy = conn.execute(sql, params)
            analysis_id = cast(UUID, result_proxy.scalar_one())
            conn.commit()
        self.logger.info(f"Pre-analysis record created successfully with ID: {analysis_id}.")
        return analysis_id

    def update_pre_analysis_with_tokens(
        self,
        analysis_id: UUID,
        input_tokens_used: int,
        output_tokens_used: int,
        thinking_tokens_used: int,
        input_cost: Decimal,
        output_cost: Decimal,
        thinking_cost: Decimal,
        search_cost: Decimal,
        total_cost: Decimal,
        search_queries_used: int = 0,
        analysis_prompt: str = "",
    ) -> None:
        """Updates an existing analysis record with token counts and costs.

        Args:
            analysis_id: The ID of the analysis record to update.
            input_tokens_used: The estimated number of input tokens.
            output_tokens_used: The estimated number of output tokens.
            thinking_tokens_used: The estimated number of thinking tokens used.
            input_cost: The calculated cost of the input tokens.
            output_cost: The calculated cost of the output tokens.
            thinking_cost: The calculated cost of the thinking tokens.
            thinking_cost: The calculated cost of the thinking tokens.
            search_cost: The calculated cost of the search queries.
            total_cost: The total calculated cost of the analysis.
            search_queries_used: The number of search queries performed.
            analysis_prompt: The prompt used for the analysis.
        """
        self.logger.info(f"Updating pre-analysis record {analysis_id} with token counts.")
        sql = text(
            """
            UPDATE procurement_analyses
            SET
                input_tokens_used = :input_tokens_used,
                output_tokens_used = :output_tokens_used,
                thinking_tokens_used = :thinking_tokens_used,
                cost_input_tokens = :cost_input_tokens,
                cost_output_tokens = :cost_output_tokens,
                cost_thinking_tokens = :cost_thinking_tokens,
                cost_search_queries = :cost_search_queries,
                search_queries_used = :search_queries_used,
                total_cost = :total_cost,
                analysis_prompt = :analysis_prompt
            WHERE analysis_id = :analysis_id;
            """
        )
        params = {
            "analysis_id": analysis_id,
            "input_tokens_used": input_tokens_used,
            "output_tokens_used": output_tokens_used,
            "thinking_tokens_used": thinking_tokens_used,
            "cost_input_tokens": input_cost,
            "cost_output_tokens": output_cost,
            "cost_thinking_tokens": thinking_cost,
            "cost_search_queries": search_cost,
            "search_queries_used": search_queries_used,
            "total_cost": total_cost,
            "analysis_prompt": analysis_prompt,
        }
        with self.engine.connect() as conn:
            conn.execute(sql, params)
            conn.commit()
        self.logger.info(f"Pre-analysis record {analysis_id} updated successfully.")

    def get_analysis_by_id(self, analysis_id: UUID) -> AnalysisResult | None:
        """Retrieves a single analysis record by its primary key.

        Args:
            analysis_id: The unique ID of the analysis to retrieve.

        Returns:
            An `AnalysisResult` object if found, otherwise `None`.
        """
        sql = text(
            """
            SELECT
                analysis_id,
                procurement_control_number,
                version_number,
                status,
                risk_score,
                risk_score_rationale,
                procurement_summary,
                analysis_summary,
                red_flags,
                seo_keywords,
                document_hash,
                original_documents_gcs_path,
                processed_documents_gcs_path,
                input_tokens_used,
                output_tokens_used,
                thinking_tokens_used,
                created_at,
                updated_at,
                retry_count,
                votes_count,
                cost_input_tokens,
                cost_output_tokens,
                cost_thinking_tokens,
                cost_search_queries,
                search_queries_used,
                total_cost,
                analysis_prompt,
                thoughts
            FROM procurement_analyses
            WHERE analysis_id = :analysis_id
            LIMIT 1;
            """
        )

        with self.engine.connect() as conn:
            result = conn.execute(sql, {"analysis_id": analysis_id}).fetchone()
            if not result:
                return None
            columns = list(result._fields)
            row = tuple(result)

        return self._parse_row_to_model(row, columns)

    def update_analysis_status(self, analysis_id: UUID, status: ProcurementAnalysisStatus) -> None:
        """Updates the status of a specific analysis record.

        This is used to track the lifecycle of an analysis, for example,
        moving it from 'PENDING_ANALYSIS' to 'ANALYSIS_IN_PROGRESS' or
        from 'ANALYSIS_IN_PROGRESS' to 'ANALYSIS_SUCCESSFUL'.

        Args:
            analysis_id: The ID of the analysis to update.
            status: The new status to set for the analysis.
        """
        self.logger.info(f"Updating status for analysis {analysis_id} to {status}.")
        sql = text(
            """
            UPDATE procurement_analyses
            SET status = :status, updated_at = now()
            WHERE analysis_id = :analysis_id;
            """
        )
        with self.engine.connect() as conn:
            conn.execute(sql, {"analysis_id": analysis_id, "status": status.value})
            conn.commit()
        self.logger.info("Analysis status updated successfully.")

    def get_analyses_to_retry(self, max_retries: int, timeout_hours: int) -> list[AnalysisResult]:
        """Retrieves a list of analyses that are eligible for a retry attempt.

        This method identifies analyses that have failed, have been stuck in
        progress, or are stuck calculating tokens for too long, and have not
        yet exceeded the maximum number of retry attempts.

        Args:
            max_retries: The maximum number of retries allowed for an analysis.
            timeout_hours: The number of hours after which a task is considered
                stale.

        Returns:
            A list of `AnalysisResult` objects that are eligible for retry.
        """
        self.logger.info("Fetching analyses to retry...")
        sql = text(
            """
            SELECT
                analysis_id,
                procurement_control_number,
                version_number,
                status,
                retry_count,
                risk_score,
                risk_score_rationale,
                procurement_summary,
                analysis_summary,
                red_flags,
                seo_keywords,
                document_hash,
                original_documents_gcs_path,
                processed_documents_gcs_path,
                input_tokens_used,
                output_tokens_used,
                thinking_tokens_used,
                created_at,
                updated_at,
                votes_count,
                cost_input_tokens,
                cost_output_tokens,
                cost_thinking_tokens,
                cost_search_queries,
                search_queries_used,
                total_cost,
                analysis_prompt,
                thoughts
            FROM procurement_analyses
            WHERE
                (
                    status = :failed_status
                    OR (
                        status = :in_progress_status
                        AND updated_at < NOW() - (INTERVAL '1 hour' * :timeout_hours)
                    )
                    OR (
                        status = :pending_token_status
                        AND updated_at < NOW() - (INTERVAL '1 hour' * :timeout_hours)
                    )
                )
                AND retry_count < :max_retries
                AND NOT EXISTS (
                    SELECT 1
                    FROM procurement_analyses pa2
                    WHERE pa2.procurement_control_number = procurement_analyses.procurement_control_number
                      AND pa2.version_number = procurement_analyses.version_number
                      AND pa2.retry_count > procurement_analyses.retry_count
                );
            """
        )
        params = {
            "failed_status": ProcurementAnalysisStatus.ANALYSIS_FAILED.value,
            "in_progress_status": ProcurementAnalysisStatus.ANALYSIS_IN_PROGRESS.value,
            "pending_token_status": ProcurementAnalysisStatus.PENDING_TOKEN_CALCULATION.value,
            "timeout_hours": timeout_hours,
            "max_retries": max_retries,
        }
        with self.engine.connect() as conn:
            result = conn.execute(sql, params).fetchall()

        if not result:
            return []

        columns = list(result[0]._fields)
        analyses = [self._parse_row_to_model(tuple(row), columns) for row in result]
        return [analysis for analysis in analyses if analysis]

    def save_retry_analysis(
        self,
        procurement_control_number: str,
        version_number: int,
        document_hash: str,
        input_tokens_used: int,
        output_tokens_used: int,
        thinking_tokens_used: int,
        input_cost: Decimal,
        output_cost: Decimal,
        thinking_cost: Decimal,
        search_cost: Decimal,
        total_cost: Decimal,
        search_queries_used: int,
        retry_count: int,
        analysis_prompt: str,
    ) -> UUID:
        """Saves a new, pending analysis record for a retry attempt.

        This method creates a new analysis record with an incremented retry
        count, setting its status to 'PENDING_ANALYSIS'.

        Args:
            procurement_control_number: The control number of the associated
                procurement.
            version_number: The version of the procurement being analyzed.
            document_hash: The hash of the documents selected for analysis.
            input_tokens_used: The estimated number of input tokens.
            output_tokens_used: The estimated number of output tokens.
            thinking_tokens_used: The estimated number of thinking tokens used.
            input_cost: The calculated cost of the input tokens.
            output_cost: The calculated cost of the output tokens.
            thinking_cost: The calculated cost of the thinking tokens.
            search_cost: The calculated cost of the search queries.
            total_cost: The total calculated cost of the analysis.
            search_queries_used: The number of search queries performed.
            retry_count: The new retry count for this analysis attempt.
            analysis_prompt: The prompt from the original analysis.

        Returns:
            The newly created `analysis_id` for the record.
        """
        self.logger.info(
            f"Saving new retry analysis for {procurement_control_number} "
            f"version {version_number} (attempt {retry_count})."
        )
        analysis_id = self.create_pre_analysis_record(
            procurement_control_number=procurement_control_number,
            version_number=version_number,
            document_hash=document_hash,
            retry_count=retry_count,
        )
        self.update_pre_analysis_with_tokens(
            analysis_id=analysis_id,
            input_tokens_used=input_tokens_used,
            output_tokens_used=output_tokens_used,
            thinking_tokens_used=thinking_tokens_used,
            input_cost=input_cost,
            output_cost=output_cost,
            thinking_cost=thinking_cost,
            search_cost=search_cost,
            total_cost=total_cost,
            search_queries_used=search_queries_used,
            analysis_prompt=analysis_prompt,
        )
        self.update_analysis_status(analysis_id, ProcurementAnalysisStatus.PENDING_ANALYSIS)
        self.logger.info(f"Retry analysis saved successfully with ID: {analysis_id}.")
        return analysis_id

    def get_pending_analyses_ranked(self) -> list[AnalysisResult]:
        """Retrieves all pending analyses, ranked by votes and cost.

        This method fetches all analyses with the 'PENDING_ANALYSIS' status,
        ordering them first by the number of votes in descending order, and
        then by the estimated input tokens (as a proxy for cost) in
        ascending order.

        Returns:
            A list of `AnalysisResult` objects for the pending analyses.
        """
        self.logger.info("Fetching pending analyses, ranked by votes and cost...")
        sql = text(
            """
            SELECT
                procurement_analyses.analysis_id,
                procurement_analyses.procurement_control_number,
                procurement_analyses.version_number,
                procurement_analyses.status,
                procurement_analyses.retry_count,
                procurement_analyses.risk_score,
                procurement_analyses.risk_score_rationale,
                procurement_analyses.procurement_summary,
                procurement_analyses.analysis_summary,
                procurement_analyses.red_flags,
                procurement_analyses.seo_keywords,
                procurement_analyses.document_hash,
                procurement_analyses.original_documents_gcs_path,
                procurement_analyses.processed_documents_gcs_path,
                procurement_analyses.analysis_prompt,
                procurement_analyses.input_tokens_used,
                procurement_analyses.output_tokens_used,
                procurement_analyses.thinking_tokens_used,
                procurement_analyses.created_at,
                procurement_analyses.updated_at,
                procurement_analyses.cost_input_tokens,
                procurement_analyses.cost_output_tokens,
                procurement_analyses.cost_thinking_tokens,
                procurement_analyses.cost_search_queries,
                procurement_analyses.search_queries_used,
                procurement_analyses.total_cost,
                procurement_analyses.thoughts,
                COUNT(votes.vote_id) AS votes_count
            FROM
                procurement_analyses
            LEFT JOIN votes ON procurement_analyses.procurement_control_number = votes.procurement_control_number
                AND procurement_analyses.version_number = votes.version_number
            WHERE procurement_analyses.status = :pending_status
            GROUP BY
                procurement_analyses.analysis_id
            ORDER BY
                votes_count DESC,
                procurement_analyses.input_tokens_used ASC;
            """
        )
        params = {"pending_status": ProcurementAnalysisStatus.PENDING_ANALYSIS.value}
        with self.engine.connect() as conn:
            result = conn.execute(sql, params).fetchall()

        if not result:
            return []

        columns = list(result[0]._fields)
        analyses = [self._parse_row_to_model(tuple(row), columns) for row in result]
        return [analysis for analysis in analyses if analysis]

    def get_procurement_overall_status(self, procurement_control_number: str) -> dict[str, Any] | None:
        """Retrieves the overall status of a procurement based on its analysis history.

        This method executes a complex query that determines the single, most relevant
        status for a procurement, considering all its versions and analysis states.

        Args:
            procurement_control_number: The unique control number of the procurement.

        Returns:
            A dictionary containing the 'procurement_id', 'latest_version', and
            'overall_status', or None if the procurement is not found.
        """
        sql = text(  # pragma: no cover
            """
            SELECT
              latest_version_status_rollup.pncp_control_number AS procurement_id,
              latest_version_status_rollup.latest_version,
              CASE
                WHEN latest_version_status_rollup.latest_version_has_in_progress THEN 'ANALYSIS_IN_PROGRESS'
                WHEN latest_version_status_rollup.latest_version_has_success     THEN 'ANALYZED_CURRENT'
                WHEN latest_version_status_rollup.latest_version_has_failed      THEN 'FAILED_CURRENT'
                WHEN
                    any_previous_version_analyzed.has_success_in_previous_versions IS TRUE
                THEN 'ANALYZED_OUTDATED'
                WHEN
                    latest_version_status_rollup.latest_version_has_pending OR
                    latest_version_status_rollup.latest_version IS NOT NULL
                THEN 'PENDING'
                ELSE 'NOT_ANALYZED'
              END AS overall_status
            FROM (
              SELECT
                latest_procurement.pncp_control_number,
                latest_procurement.latest_version,
                COALESCE(analysis_status_per_version.version_has_success, false)     AS latest_version_has_success,
                COALESCE(analysis_status_per_version.version_has_in_progress, false) AS latest_version_has_in_progress,
                COALESCE(analysis_status_per_version.version_has_failed, false)      AS latest_version_has_failed,
                COALESCE(analysis_status_per_version.version_has_pending, false)     AS latest_version_has_pending
              FROM (
                SELECT
                  pncp_control_number,
                  MAX(version_number) AS latest_version
                FROM procurements
                WHERE pncp_control_number = :pncp_control_number
                GROUP BY pncp_control_number
              ) AS latest_procurement
              LEFT JOIN (
                SELECT
                  procurement_analyses.procurement_control_number,
                  procurement_analyses.version_number,
                  BOOL_OR(procurement_analyses.status::text = 'ANALYSIS_SUCCESSFUL') AS version_has_success,
                  BOOL_OR(procurement_analyses.status::text = 'ANALYSIS_IN_PROGRESS') AS version_has_in_progress,
                  BOOL_OR(procurement_analyses.status::text = 'ANALYSIS_FAILED')     AS version_has_failed,
                  BOOL_OR(procurement_analyses.status::text = 'PENDING_ANALYSIS')    AS version_has_pending
                FROM procurement_analyses
                WHERE procurement_analyses.procurement_control_number = :pncp_control_number
                GROUP BY
                  procurement_analyses.procurement_control_number,
                  procurement_analyses.version_number
              ) AS analysis_status_per_version
                ON analysis_status_per_version.procurement_control_number = latest_procurement.pncp_control_number
               AND analysis_status_per_version.version_number = latest_procurement.latest_version
            ) AS latest_version_status_rollup
            LEFT JOIN (
              SELECT
                latest_procurement.pncp_control_number,
                BOOL_OR(analysis_status_per_version.version_has_success) AS has_success_in_previous_versions
              FROM (
                SELECT
                  pncp_control_number,
                  MAX(version_number) AS latest_version
                FROM procurements
                WHERE pncp_control_number = :pncp_control_number
                GROUP BY pncp_control_number
              ) AS latest_procurement
              JOIN (
                SELECT
                  procurement_analyses.procurement_control_number,
                  procurement_analyses.version_number,
                  BOOL_OR(procurement_analyses.status::text = 'ANALYSIS_SUCCESSFUL') AS version_has_success
                FROM procurement_analyses
                WHERE procurement_analyses.procurement_control_number = :pncp_control_number
                GROUP BY
                  procurement_analyses.procurement_control_number,
                  procurement_analyses.version_number
              ) AS analysis_status_per_version
                ON analysis_status_per_version.procurement_control_number = latest_procurement.pncp_control_number
               AND analysis_status_per_version.version_number < latest_procurement.latest_version
              GROUP BY latest_procurement.pncp_control_number
            ) AS any_previous_version_analyzed
              ON any_previous_version_analyzed.pncp_control_number = latest_version_status_rollup.pncp_control_number;
            """
        )
        with self.engine.connect() as conn:
            result = conn.execute(sql, {"pncp_control_number": procurement_control_number}).fetchone()

        if not result:
            return None

        return dict(result._mapping)

    def get_analysis_by_id(self, analysis_id: UUID) -> dict[str, Any] | None:
        """Retrieves a single analysis by ID, including procurement details."""
        sql = text("""
            SELECT 
                pa.*,
                p.total_estimated_value,
                p.pncp_publication_date,
                p.modality_id,
                p.procurement_status_id,
                p.raw_data
            FROM procurement_analyses pa
            JOIN procurements p ON pa.procurement_control_number = p.pncp_control_number AND pa.version_number = p.version_number
            WHERE pa.analysis_id = :analysis_id
        """)
        with self.engine.connect() as conn:
            result = conn.execute(sql, {"analysis_id": analysis_id}).fetchone()
            
        if not result:
            return None
            
        return dict(result._mapping)


    def get_home_stats(self) -> dict[str, Any]:
        """Retrieves statistics for the home page."""
        with self.engine.connect() as conn:
            total_analyses = conn.execute(
                text("SELECT COUNT(*) FROM procurement_analyses WHERE status = :status"),
                {"status": ProcurementAnalysisStatus.ANALYSIS_SUCCESSFUL.value}
            ).scalar() or 0
            
            high_risk_count = conn.execute(
                text("SELECT COUNT(*) FROM procurement_analyses WHERE status = :status AND risk_score > 70"),
                {"status": ProcurementAnalysisStatus.ANALYSIS_SUCCESSFUL.value}
            ).scalar() or 0
            total_savings = conn.execute(
                text("""
                    SELECT SUM((elem->>'potential_savings')::numeric)
                    FROM procurement_analyses pa,
                         jsonb_array_elements(pa.red_flags) elem
                    WHERE pa.status = :status
                """),
                {"status": ProcurementAnalysisStatus.ANALYSIS_SUCCESSFUL.value}
            ).scalar() or 0
            
            return {
                "total_analyses": total_analyses,
                "high_risk_count": high_risk_count,
                "total_savings": total_savings
            }

    def get_recent_analyses_summary(self, page: int = 1, limit: int = 9) -> tuple[list[AnalysisResult], int]:
        """Retrieves recent successful analyses with procurement details, paginated."""
        offset = (page - 1) * limit
        
        # Count query
        count_sql = text("""
            SELECT COUNT(*) 
            FROM procurement_analyses 
            WHERE status = :status
        """)
        
        # Data query
        sql = text("""
            SELECT 
                pa.*,
                p.raw_data,
                COALESCE((
                    SELECT SUM((elem->>'potential_savings')::numeric)
                    FROM jsonb_array_elements(pa.red_flags) elem
                ), 0) as total_savings_calc
            FROM procurement_analyses pa
            JOIN procurements p ON pa.procurement_control_number = p.pncp_control_number 
                                AND pa.version_number = p.version_number
            WHERE pa.status = :status 
            ORDER BY pa.risk_score DESC NULLS LAST, total_savings_calc DESC
            LIMIT :limit OFFSET :offset
        """)
        
        with self.engine.connect() as conn:
            total_count = conn.execute(count_sql, {"status": ProcurementAnalysisStatus.ANALYSIS_SUCCESSFUL.value}).scalar()
            result = conn.execute(sql, {
                "status": ProcurementAnalysisStatus.ANALYSIS_SUCCESSFUL.value, 
                "limit": limit,
                "offset": offset
            }).fetchall()
        
        if not result:
            return [], total_count or 0
            
        columns = list(result[0]._fields)
        return [res for row in result if (res := self._parse_row_to_model(tuple(row), columns))], total_count or 0

    def search_analyses_summary(self, query: str, page: int = 1, limit: int = 9) -> tuple[list[AnalysisResult], int]:
        """Searches analyses by summary or control number, paginated."""
        offset = (page - 1) * limit
        
        # Count query
        count_sql = text("""
            SELECT COUNT(*)
            FROM procurement_analyses pa
            WHERE pa.status = :status 
            AND (
                pa.procurement_summary ILIKE :q_summary 
                OR pa.analysis_summary ILIKE :q_summary
                OR pa.procurement_control_number ILIKE :q_control_number
            )
        """)
        
        # Data query
        sql = text("""
            SELECT 
                pa.*,
                p.raw_data,
                COALESCE((
                    SELECT SUM((elem->>'potential_savings')::numeric)
                    FROM jsonb_array_elements(pa.red_flags) elem
                ), 0) as total_savings_calc
            FROM procurement_analyses pa
            JOIN procurements p ON pa.procurement_control_number = p.pncp_control_number 
                                AND pa.version_number = p.version_number
            WHERE pa.status = :status 
            AND (
                procurement_summary ILIKE :q_summary
                OR pa.procurement_control_number ILIKE :q_control_number
            )
            ORDER BY pa.risk_score DESC NULLS LAST, total_savings_calc DESC
            LIMIT :limit OFFSET :offset
        """)
        
        params = {
            "status": ProcurementAnalysisStatus.ANALYSIS_SUCCESSFUL.value,
            "q_summary": f"%{query}%",
            "q_control_number": f"%{query}%",
            "limit": limit,
            "offset": offset
        }
        
        with self.engine.connect() as conn:
            total_count = conn.execute(count_sql, {
                "status": ProcurementAnalysisStatus.ANALYSIS_SUCCESSFUL.value,
                "q_summary": f"%{query}%",
                "q_control_number": f"%{query}%"
            }).scalar()
            result = conn.execute(sql, params).fetchall()
            
        if not result:
            return [], total_count or 0
            
        columns = list(result[0]._fields)
        return [res for row in result if (res := self._parse_row_to_model(tuple(row), columns))], total_count or 0
